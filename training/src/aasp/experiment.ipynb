{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aca5eee",
   "metadata": {},
   "source": [
    "## Claude's Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d69b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19202141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "\n",
    "# Load the pickle file\n",
    "with open('../../../data/train/combined_train_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Convert to DataFrame if it's not already\n",
    "if not isinstance(data, pd.DataFrame):\n",
    "    df = pd.DataFrame(data)\n",
    "else:\n",
    "    df = data\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(df['score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2.1\n",
    "\n",
    "# # Filter out extreme outliers MORE aggressively\n",
    "# print(f\"Original dataset size: {len(df)}\")\n",
    "\n",
    "# # Keep only values within a reasonable range (95th percentile and below)\n",
    "# lower_percentile = df['score'].quantile(0.05)\n",
    "# upper_percentile = df['score'].quantile(0.95)\n",
    "\n",
    "# df_filtered = df[(df['score'] >= lower_percentile) & (df['score'] <= upper_percentile)].copy()\n",
    "\n",
    "# print(f\"Filtered dataset size: {len(df_filtered)}\")\n",
    "# print(f\"Removed {len(df) - len(df_filtered)} outliers ({100*(len(df) - len(df_filtered))/len(df):.1f}%)\")\n",
    "# print(f\"New score range: {df_filtered['score'].min():.4f} to {df_filtered['score'].max():.4f}\")\n",
    "# print(f\"New score mean: {df_filtered['score'].mean():.4f}, std: {df_filtered['score'].std():.4f}\")\n",
    "\n",
    "# # Use filtered dataframe\n",
    "# df = df_filtered\n",
    "\n",
    "# # Filter only the most extreme outliers (keep 98% of data)\n",
    "# print(f\"Original dataset size: {len(df)}\")\n",
    "\n",
    "# # Keep values between 1st and 99th percentile\n",
    "# lower_percentile = df['score'].quantile(0.01)\n",
    "# upper_percentile = df['score'].quantile(0.99)\n",
    "\n",
    "# df_filtered = df[(df['score'] >= lower_percentile) & (df['score'] <= upper_percentile)].copy()\n",
    "\n",
    "# print(f\"Filtered dataset size: {len(df_filtered)}\")\n",
    "# print(f\"Removed {len(df) - len(df_filtered)} extreme outliers ({100*(len(df) - len(df_filtered))/len(df):.1f}%)\")\n",
    "# print(f\"New score range: {df_filtered['score'].min():.4f} to {df_filtered['score'].max():.4f}\")\n",
    "# print(f\"New score mean: {df_filtered['score'].mean():.4f}, std: {df_filtered['score'].std():.4f}\")\n",
    "\n",
    "# # Use filtered dataframe\n",
    "# df = df_filtered\n",
    "\n",
    "# NO FILTERING - use all data including extreme outliers\n",
    "print(f\"Using FULL unfiltered dataset: {len(df)} samples\")\n",
    "print(f\"Score range: {df['score'].min():.4f} to {df['score'].max():.4f}\")\n",
    "print(f\"Score mean: {df['score'].mean():.4f}, std: {df['score'].std():.4f}\")\n",
    "print(f\"\\nScore percentiles:\")\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    print(f\"  {p}th: {df['score'].quantile(p/100):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean any corrupted embeddings in the dataset\n",
    "print(\"Checking for corrupted embeddings...\")\n",
    "bad_indices = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    ref_emb = df['ref_embedding'].iloc[idx]\n",
    "    alt_emb = df['alt_embedding'].iloc[idx]\n",
    "    \n",
    "    # Convert to tensor to check\n",
    "    if not isinstance(ref_emb, torch.Tensor):\n",
    "        ref_emb = torch.tensor(ref_emb)\n",
    "    if not isinstance(alt_emb, torch.Tensor):\n",
    "        alt_emb = torch.tensor(alt_emb)\n",
    "    \n",
    "    # Check for NaN or Inf\n",
    "    if torch.isnan(ref_emb).any() or torch.isinf(ref_emb).any():\n",
    "        bad_indices.append(idx)\n",
    "    elif torch.isnan(alt_emb).any() or torch.isinf(alt_emb).any():\n",
    "        bad_indices.append(idx)\n",
    "\n",
    "print(f\"Found {len(bad_indices)} corrupted embeddings ({100*len(bad_indices)/len(df):.2f}%)\")\n",
    "\n",
    "if len(bad_indices) > 0:\n",
    "    print(f\"Removing corrupted entries...\")\n",
    "    df = df.drop(bad_indices).reset_index(drop=True)\n",
    "    print(f\"Clean dataset size: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "\n",
    "# Check the shape of embeddings\n",
    "sample_ref_emb = df['ref_embedding'].iloc[0]\n",
    "sample_alt_emb = df['alt_embedding'].iloc[0]\n",
    "\n",
    "print(f\"Reference embedding shape: {sample_ref_emb.shape}\")\n",
    "print(f\"Alternative embedding shape: {sample_alt_emb.shape}\")\n",
    "print(f\"Reference embedding type: {type(sample_ref_emb)}\")\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dim = sample_ref_emb.shape[0]\n",
    "print(f\"\\nEmbedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1\n",
    "\n",
    "# Let's see the actual score distribution\n",
    "print(\"Raw score statistics:\")\n",
    "print(f\"Min: {df['score'].min()}\")\n",
    "print(f\"Max: {df['score'].max()}\")\n",
    "print(f\"Mean: {df['score'].mean()}\")\n",
    "print(f\"Std: {df['score'].std()}\")\n",
    "print(f\"Median: {df['score'].median()}\")\n",
    "\n",
    "# Check for outliers\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    print(f\"{p}th: {df['score'].quantile(p/100):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2 ‚Äì Build categorical feature mappings\n",
    "\n",
    "# Categorical columns to use\n",
    "cat_columns = ['accession', 'scoreset', 'ensp', 'ref_long', 'alt_long', 'biotype']\n",
    "\n",
    "cat_maps = {}\n",
    "for col in cat_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "    uniques = df[col].dropna().unique()\n",
    "    cat_maps[col] = {val: idx for idx, val in enumerate(uniques)}\n",
    "    print(f\"Column '{col}' -> {len(uniques)} unique categories\")\n",
    "\n",
    "print(\"\\nCategorical mappings created for:\", cat_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a23435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# class ProteinScoreDataset(Dataset):\n",
    "#     def __init__(self, dataframe, score_scaler=None, fit_scaler=False):\n",
    "#         self.df = dataframe.reset_index(drop=True)\n",
    "        \n",
    "#         # Use MinMaxScaler to scale to [0, 1] - more stable than StandardScaler\n",
    "#         if fit_scaler:\n",
    "#             self.score_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#             scores = self.df['score'].values.reshape(-1, 1)\n",
    "#             self.score_scaler.fit(scores)\n",
    "#         else:\n",
    "#             self.score_scaler = score_scaler\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # Get embeddings\n",
    "#         ref_embedding = row['ref_embedding']\n",
    "#         alt_embedding = row['alt_embedding']\n",
    "        \n",
    "#         # Ensure embeddings are torch tensors\n",
    "#         if not isinstance(ref_embedding, torch.Tensor):\n",
    "#             ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             ref_embedding = ref_embedding.float()\n",
    "            \n",
    "#         if not isinstance(alt_embedding, torch.Tensor):\n",
    "#             alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             alt_embedding = alt_embedding.float()\n",
    "        \n",
    "#         # Concatenate embeddings\n",
    "#         diff = alt_embedding - ref_embedding   ### CHANGED\n",
    "#         abs_diff = torch.abs(diff)   ### CHANGED\n",
    "#         combined_embedding = torch.cat([diff, abs_diff], dim=0)   ### CHANGED\n",
    "        \n",
    "#         # Normalize embeddings (L2 normalization)\n",
    "#         norm = torch.norm(combined_embedding)\n",
    "#         if norm > 0:\n",
    "#             combined_embedding = combined_embedding / norm\n",
    "        \n",
    "#         # Get normalized score (will be in [0, 1] range)\n",
    "#         score = self.score_scaler.transform([[row['score']]])[0][0]\n",
    "#         score = torch.tensor(score, dtype=torch.float32)\n",
    "        \n",
    "#         return combined_embedding, score\n",
    "\n",
    "\n",
    "# # ----- NEURAL NET ----- #\n",
    "\n",
    "# from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# class ProteinScoreDataset(Dataset):\n",
    "#     def __init__(self, dataframe, score_scaler=None, fit_scaler=False):\n",
    "#         self.df = dataframe.reset_index(drop=True)\n",
    "        \n",
    "#         # Calculate shift value to make all scores positive\n",
    "#         self.score_shift = abs(self.df['score'].min()) + 1\n",
    "        \n",
    "#         # Use RobustScaler on LOG-TRANSFORMED scores\n",
    "#         if fit_scaler:\n",
    "#             self.score_scaler = RobustScaler()\n",
    "#             # Log transform scores\n",
    "#             scores_shifted = self.df['score'].values + self.score_shift\n",
    "#             scores_log = np.log(scores_shifted)\n",
    "#             self.score_scaler.fit(scores_log.reshape(-1, 1))\n",
    "#         else:\n",
    "#             self.score_scaler = score_scaler\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.df.iloc[idx]\n",
    "        \n",
    "    #     # Get embeddings\n",
    "    #     ref_embedding = row['ref_embedding']\n",
    "    #     alt_embedding = row['alt_embedding']\n",
    "        \n",
    "    #     # Ensure embeddings are torch tensors\n",
    "    #     if not isinstance(ref_embedding, torch.Tensor):\n",
    "    #         ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "    #     else:\n",
    "    #         ref_embedding = ref_embedding.float()\n",
    "            \n",
    "    #     if not isinstance(alt_embedding, torch.Tensor):\n",
    "    #         alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "    #     else:\n",
    "    #         alt_embedding = alt_embedding.float()\n",
    "        \n",
    "    #     # USE DIFFERENCE instead of concatenation\n",
    "    #     diff = alt_embedding - ref_embedding\n",
    "    #     abs_diff = torch.abs(diff)\n",
    "    #     combined_embedding = torch.cat([diff, abs_diff], dim=0)\n",
    "        \n",
    "    #     # Normalize the combined features\n",
    "    #     norm = torch.norm(combined_embedding)\n",
    "    #     if norm > 0:\n",
    "    #         combined_embedding = combined_embedding / norm\n",
    "        \n",
    "    #     # Log transform and normalize score\n",
    "    #     score_shifted = row['score'] + self.score_shift\n",
    "    #     score_log = np.log(score_shifted)\n",
    "    #     score_normalized = self.score_scaler.transform([[score_log]])[0][0]\n",
    "    #     score = torch.tensor(score_normalized, dtype=torch.float32)\n",
    "        \n",
    "    #     return combined_embedding, score\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.df.iloc[idx]\n",
    "\n",
    "    #     # ---- embeddings (numeric feature we can use immediately) ----\n",
    "    #     ref_embedding = row[\"ref_embedding\"]\n",
    "    #     alt_embedding = row[\"alt_embedding\"]\n",
    "\n",
    "    #     if not isinstance(ref_embedding, torch.Tensor):\n",
    "    #         ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "    #     else:\n",
    "    #         ref_embedding = ref_embedding.float()\n",
    "\n",
    "    #     if not isinstance(alt_embedding, torch.Tensor):\n",
    "    #         alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "    #     else:\n",
    "    #         alt_embedding = alt_embedding.float()\n",
    "\n",
    "    #     # diff + |diff|, then L2-normalize\n",
    "    #     diff = alt_embedding - ref_embedding\n",
    "    #     abs_diff = torch.abs(diff)\n",
    "    #     combined_embedding = torch.cat([diff, abs_diff], dim=0)\n",
    "    #     norm = torch.norm(combined_embedding)\n",
    "    #     if norm > 0:\n",
    "    #         combined_embedding = combined_embedding / norm\n",
    "\n",
    "    #     # ---- target (same normalization you already had) ----\n",
    "    #     score_shifted = row[\"score\"] + self.score_shift\n",
    "    #     score_log = np.log(score_shifted)\n",
    "    #     score_normalized = self.score_scaler.transform([[score_log]])[0][0]\n",
    "    #     score = torch.tensor(score_normalized, dtype=torch.float32)\n",
    "\n",
    "    #     # ---- pack ALL other columns; keep raw so you can encode later ----\n",
    "    #     features = {\n",
    "    #         # numeric tensor you can feed straight into a model head\n",
    "    #         \"X\": combined_embedding,                    # torch.FloatTensor [2*D]\n",
    "\n",
    "    #         # raw meta / categorical fields (encode later)\n",
    "    #         \"accession\": str(row[\"accession\"]),\n",
    "    #         \"scoreset\":  str(row[\"scoreset\"]),\n",
    "    #         \"ensp\":      str(row[\"ensp\"]),\n",
    "    #         \"pos\":       torch.tensor(int(row[\"pos\"]), dtype=torch.float32),  # keep as tensor\n",
    "    #         \"ref_long\":  str(row[\"ref_long\"]),\n",
    "    #         \"alt_long\":  str(row[\"alt_long\"]),\n",
    "    #         \"biotype\":   str(row[\"biotype\"]),\n",
    "    #         # list[str]; default collate will give you a list-of-lists per batch\n",
    "    #         \"consequence\": list(row[\"consequence\"]) if isinstance(row[\"consequence\"], (list, tuple))\n",
    "    #                     else ([row[\"consequence\"]] if row[\"consequence\"] is not None else []),\n",
    "    #     }\n",
    "\n",
    "    #     # Return a (features, target) pair; DataLoader can collate dicts by key.\n",
    "    #     return features, score\n",
    "\n",
    "    \n",
    "    # def inverse_transform_score(self, normalized_scores):\n",
    "    #     \"\"\"Convert normalized log scores back to original scale\"\"\"\n",
    "    #     # Input: normalized log-transformed scores\n",
    "    #     # Output: original scale scores\n",
    "        \n",
    "    #     normalized_scores = np.array(normalized_scores).reshape(-1, 1)\n",
    "        \n",
    "    #     # Step 1: Inverse normalize\n",
    "    #     scores_log = self.score_scaler.inverse_transform(normalized_scores)\n",
    "        \n",
    "    #     # Step 2: Inverse log transform\n",
    "    #     scores_shifted = np.exp(scores_log)\n",
    "        \n",
    "    #     # Step 3: Remove shift\n",
    "    #     scores_original = scores_shifted - self.score_shift\n",
    "        \n",
    "    #     return scores_original.flatten()\n",
    "\n",
    "# Cell 4 ‚Äì Dataset with full feature vector (embeddings + other columns)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "class ProteinScoreDataset(Dataset):\n",
    "    def __init__(self, dataframe, score_scaler=None, fit_scaler=False,\n",
    "                 cat_columns=None, cat_maps=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.cat_columns = cat_columns or []\n",
    "        self.cat_maps = cat_maps or {}\n",
    "\n",
    "        # Shift to make all scores positive\n",
    "        self.score_shift = abs(self.df['score'].min()) + 1\n",
    "\n",
    "        # RobustScaler on LOG-transformed scores\n",
    "        if fit_scaler:\n",
    "            self.score_scaler = RobustScaler()\n",
    "            scores_shifted = self.df['score'].values + self.score_shift\n",
    "            scores_log = np.log(scores_shifted)\n",
    "            self.score_scaler.fit(scores_log.reshape(-1, 1))\n",
    "        else:\n",
    "            self.score_scaler = score_scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ---- Embeddings ----\n",
    "        ref_embedding = row[\"ref_embedding\"]\n",
    "        alt_embedding = row[\"alt_embedding\"]\n",
    "\n",
    "        if not isinstance(ref_embedding, torch.Tensor):\n",
    "            ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "        else:\n",
    "            ref_embedding = ref_embedding.float()\n",
    "\n",
    "        if not isinstance(alt_embedding, torch.Tensor):\n",
    "            alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "        else:\n",
    "            alt_embedding = alt_embedding.float()\n",
    "\n",
    "        # diff + |diff|, then L2-normalize\n",
    "        diff = alt_embedding - ref_embedding\n",
    "        abs_diff = torch.abs(diff)\n",
    "        combined_embedding = torch.cat([diff, abs_diff], dim=0)   # [2*D]\n",
    "\n",
    "        norm = torch.norm(combined_embedding)\n",
    "        if norm > 0:\n",
    "            combined_embedding = combined_embedding / norm\n",
    "\n",
    "        # ---- Extra features: categorical + pos ----\n",
    "        cat_feats = []\n",
    "        for col in self.cat_columns:\n",
    "            val = str(row[col])\n",
    "            mapping = self.cat_maps.get(col, {})\n",
    "            cat_feats.append(mapping.get(val, -1))  # -1 for unseen\n",
    "\n",
    "        pos_val = row['pos']\n",
    "        if pd.isna(pos_val) or np.isinf(pos_val):\n",
    "            pos_val = -1\n",
    "        extra_features = torch.tensor(cat_feats + [float(pos_val)],\n",
    "                                      dtype=torch.float32)        # [len(cat_columns)+1]\n",
    "\n",
    "        # Final input vector: [2*D + len(cat_columns) + 1]\n",
    "        X = torch.cat([combined_embedding, extra_features], dim=0)\n",
    "\n",
    "        # ---- Target (log + RobustScaler) ----\n",
    "        score_shifted = row[\"score\"] + self.score_shift\n",
    "        score_log = np.log(score_shifted)\n",
    "        score_normalized = self.score_scaler.transform([[score_log]])[0][0]\n",
    "        y = torch.tensor(score_normalized, dtype=torch.float32)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def inverse_transform_score(self, normalized_scores):\n",
    "        \"\"\"Convert normalized log scores back to original scale.\"\"\"\n",
    "        normalized_scores = np.array(normalized_scores).reshape(-1, 1)\n",
    "\n",
    "        # Inverse RobustScaler\n",
    "        scores_log = self.score_scaler.inverse_transform(normalized_scores)\n",
    "\n",
    "        # Inverse log\n",
    "        scores_shifted = np.exp(scores_log)\n",
    "\n",
    "        # Remove shift\n",
    "        scores_original = scores_shifted - self.score_shift\n",
    "\n",
    "        return scores_original.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 5\n",
    "\n",
    "# # Split data into train and validation sets\n",
    "# train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(f\"Training set size: {len(train_df)}\")\n",
    "# print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# # Create datasets with normalization\n",
    "# train_dataset = ProteinScoreDataset(train_df, fit_scaler=True)\n",
    "# val_dataset = ProteinScoreDataset(val_df, score_scaler=train_dataset.score_scaler, fit_scaler=False)\n",
    "\n",
    "# # Create dataloaders\n",
    "# batch_size = 64\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# print(f\"\\nScore normalization (RobustScaler):\")\n",
    "# print(f\"  Center: {train_dataset.score_scaler.center_[0]:.4f}\")\n",
    "# print(f\"  Scale: {train_dataset.score_scaler.scale_[0]:.4f}\")\n",
    "\n",
    "# Cell 5\n",
    "\n",
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Create datasets with normalization + extra features\n",
    "train_dataset = ProteinScoreDataset(\n",
    "    train_df,\n",
    "    fit_scaler=True,\n",
    "    cat_columns=cat_columns,\n",
    "    cat_maps=cat_maps\n",
    ")\n",
    "\n",
    "val_dataset = ProteinScoreDataset(\n",
    "    val_df,\n",
    "    score_scaler=train_dataset.score_scaler,\n",
    "    fit_scaler=False,\n",
    "    cat_columns=cat_columns,\n",
    "    cat_maps=cat_maps\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nScore normalization (RobustScaler):\")\n",
    "print(f\"  Center: {train_dataset.score_scaler.center_[0]:.4f}\")\n",
    "print(f\"  Scale:  {train_dataset.score_scaler.scale_[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6\n",
    "\n",
    "# class ScorePredictionNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.1):\n",
    "#         super(ScorePredictionNet, self).__init__()\n",
    "        \n",
    "#         self.layers = nn.ModuleList()\n",
    "#         prev_dim = input_dim\n",
    "        \n",
    "#         # Build hidden layers\n",
    "#         for hidden_dim in hidden_dims:\n",
    "#             self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "#             self.layers.append(nn.LayerNorm(hidden_dim))  # LayerNorm instead of BatchNorm\n",
    "#             self.layers.append(nn.Tanh())  # Tanh instead of ReLU - bounded output\n",
    "#             self.layers.append(nn.Dropout(dropout_rate))\n",
    "#             prev_dim = hidden_dim\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.output = nn.Linear(prev_dim, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()  # Sigmoid to ensure [0, 1] output\n",
    "        \n",
    "#         # Initialize weights conservatively\n",
    "#         for layer in self.layers:\n",
    "#             if isinstance(layer, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(layer.weight, gain=0.1)\n",
    "#                 nn.init.zeros_(layer.bias)\n",
    "        \n",
    "#         nn.init.xavier_uniform_(self.output.weight, gain=0.01)\n",
    "#         nn.init.zeros_(self.output.bias)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "        \n",
    "#         x = self.output(x)\n",
    "#         x = self.sigmoid(x)\n",
    "        \n",
    "#         return x.squeeze()\n",
    "\n",
    "# # Initialize model\n",
    "# input_dim = embedding_dim * 2\n",
    "# model = ScorePredictionNet(input_dim)\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(f\"Using device: {device}\")\n",
    "# print(f\"\\nModel architecture:\")\n",
    "# print(model)\n",
    "# print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "# ----- NEURAL NET ----- #\n",
    "\n",
    "# # In Cell 6, modify the model:\n",
    "# class ScorePredictionNet(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=128, dropout_rate=0.2):\n",
    "#         super(ScorePredictionNet, self).__init__()\n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "#         # Add a learnable global bias term\n",
    "#         self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "#         # ... initialization ...\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = torch.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x) + self.global_bias  # Add global bias\n",
    "#         return x.squeeze()\n",
    "\n",
    "# # Initialize model\n",
    "# input_dim = embedding_dim * 2  # diff + abs_diff\n",
    "# model = ScorePredictionNet(input_dim, hidden_dim=128)  # Changed from 64\n",
    "\n",
    "# # Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = model.to(device)\n",
    "\n",
    "# print(f\"Using device: {device}\")\n",
    "# print(f\"\\nModel architecture:\")\n",
    "# print(model)\n",
    "# print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Cell 6 ‚Äì model with expanded input dim\n",
    "\n",
    "class ScorePredictionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout_rate=0.2):\n",
    "        super(ScorePredictionNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Learnable global bias\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) + self.global_bias\n",
    "        return x.squeeze()\n",
    "\n",
    "# New input_dim: embeddings (2*D) + categorical + pos\n",
    "input_dim = embedding_dim * 2 + len(cat_columns) + 1\n",
    "\n",
    "model = ScorePredictionNet(input_dim, hidden_dim=128)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aef865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "\n",
    "# # Training hyperparameters\n",
    "# learning_rate = 0.0001  # Can be slightly higher with [0,1] range\n",
    "# num_epochs = 100\n",
    "# patience = 15\n",
    "\n",
    "# # MSE is fine for [0, 1] range\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# # Learning rate scheduler\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=5\n",
    "# )\n",
    "\n",
    "# print(f\"Learning rate: {learning_rate}\")\n",
    "# print(f\"Number of epochs: {num_epochs}\")\n",
    "# print(f\"Loss function: MSE (scores in [0, 1] range)\")\n",
    "\n",
    "# ----- NEURAL NET ----- #\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "patience = 15\n",
    "\n",
    "# Huber Loss - robust to outliers\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Loss function: Huber Loss (robust to outliers)\")\n",
    "print(f\"Score transformation: Log transform + RobustScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.1\n",
    "\n",
    "# Test a single batch to see where NaN originates\n",
    "model.eval()\n",
    "train_iter = iter(train_loader)\n",
    "embeddings, scores = next(train_iter)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DEBUGGING NaN ISSUE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check input data\n",
    "print(f\"\\n1. INPUT DATA CHECK:\")\n",
    "print(f\"   Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Embeddings contain NaN: {torch.isnan(embeddings).any()}\")\n",
    "print(f\"   Embeddings contain Inf: {torch.isinf(embeddings).any()}\")\n",
    "print(f\"   Embeddings min: {embeddings.min():.4f}, max: {embeddings.max():.4f}\")\n",
    "print(f\"   Embeddings mean: {embeddings.mean():.4f}, std: {embeddings.std():.4f}\")\n",
    "\n",
    "print(f\"\\n   Scores shape: {scores.shape}\")\n",
    "print(f\"   Scores contain NaN: {torch.isnan(scores).any()}\")\n",
    "print(f\"   Scores contain Inf: {torch.isinf(scores).any()}\")\n",
    "print(f\"   Scores min: {scores.min():.4f}, max: {scores.max():.4f}\")\n",
    "print(f\"   Scores mean: {scores.mean():.4f}, std: {scores.std():.4f}\")\n",
    "\n",
    "# Check model forward pass\n",
    "embeddings = embeddings.to(device)\n",
    "scores = scores.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(embeddings)\n",
    "    \n",
    "print(f\"\\n2. MODEL OUTPUT CHECK:\")\n",
    "print(f\"   Predictions contain NaN: {torch.isnan(predictions).any()}\")\n",
    "print(f\"   Predictions contain Inf: {torch.isinf(predictions).any()}\")\n",
    "if not torch.isnan(predictions).any():\n",
    "    print(f\"   Predictions min: {predictions.min():.4f}, max: {predictions.max():.4f}\")\n",
    "    print(f\"   Predictions mean: {predictions.mean():.4f}, std: {predictions.std():.4f}\")\n",
    "\n",
    "# Check loss calculation\n",
    "loss = criterion(predictions, scores)\n",
    "print(f\"\\n3. LOSS CHECK:\")\n",
    "print(f\"   Loss value: {loss.item()}\")\n",
    "print(f\"   Loss is NaN: {torch.isnan(loss)}\")\n",
    "\n",
    "# Check model weights\n",
    "print(f\"\\n4. MODEL WEIGHTS CHECK:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"   NaN found in {name}\")\n",
    "    if torch.isinf(param).any():\n",
    "        print(f\"   Inf found in {name}\")\n",
    "    print(f\"   {name}: min={param.min():.4f}, max={param.max():.4f}, mean={param.abs().mean():.4f}\")\n",
    "    if len(list(model.named_parameters())) > 10:  # Only print first few if many layers\n",
    "        break\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_losses_denorm = []\n",
    "val_losses_denorm = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_batches = 0\n",
    "    all_train_preds = []\n",
    "    all_train_targets = []\n",
    "    \n",
    "    for embeddings, scores in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        embeddings = embeddings.to(device)\n",
    "        scores = scores.to(device)\n",
    "        \n",
    "        # Check for NaN in inputs BEFORE forward pass\n",
    "        if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():\n",
    "            print(f\"\\n‚ö†Ô∏è  NaN/Inf in embeddings at batch {train_batches}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        if torch.isnan(scores).any() or torch.isinf(scores).any():\n",
    "            print(f\"\\n‚ö†Ô∏è  NaN/Inf in scores at batch {train_batches}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(embeddings)\n",
    "        \n",
    "        # Check predictions before loss\n",
    "        if torch.isnan(predictions).any() or torch.isinf(predictions).any():\n",
    "            print(f\"\\n‚ö†Ô∏è  NaN/Inf in predictions at batch {train_batches}, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(predictions, scores)\n",
    "        \n",
    "        # Check for NaN loss before backward\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"\\n‚ö†Ô∏è  NaN loss detected at epoch {epoch+1}, batch {train_batches}\")\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_batches += 1\n",
    "        \n",
    "        # Store for denormalized metrics\n",
    "        all_train_preds.extend(predictions.detach().cpu().numpy())\n",
    "        all_train_targets.extend(scores.cpu().numpy())\n",
    "    \n",
    "    if train_batches == 0:\n",
    "        print(\"All batches had NaN loss. Stopping training.\")\n",
    "        break\n",
    "    \n",
    "    avg_train_loss = train_loss / train_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Calculate denormalized train MSE\n",
    "    train_preds_denorm = train_dataset.inverse_transform_score(all_train_preds)\n",
    "    train_targets_denorm = train_dataset.score_scaler.inverse_transform(\n",
    "        np.array(all_train_targets).reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    train_mse_denorm = mean_squared_error(train_targets_denorm, train_preds_denorm)\n",
    "    train_losses_denorm.append(train_mse_denorm)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_batches = 0\n",
    "    all_val_preds = []\n",
    "    all_val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for embeddings, scores in val_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            scores = scores.to(device)\n",
    "            \n",
    "            # Skip batches with NaN\n",
    "            if torch.isnan(embeddings).any() or torch.isnan(scores).any():\n",
    "                continue\n",
    "            \n",
    "            predictions = model(embeddings)\n",
    "            \n",
    "            if torch.isnan(predictions).any():\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(predictions, scores)\n",
    "            \n",
    "            if not torch.isnan(loss):\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Store for denormalized metrics\n",
    "                all_val_preds.extend(predictions.cpu().numpy())\n",
    "                all_val_targets.extend(scores.cpu().numpy())\n",
    "    \n",
    "    if val_batches == 0:\n",
    "        print(\"All validation batches had NaN loss. Stopping training.\")\n",
    "        break\n",
    "        \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Calculate denormalized val MSE\n",
    "    val_preds_denorm = train_dataset.inverse_transform_score(all_val_preds)\n",
    "    val_targets_denorm = train_dataset.inverse_transform_score(all_val_targets)\n",
    "    val_targets_denorm = train_dataset.score_scaler.inverse_transform(\n",
    "        np.array(all_val_targets).reshape(-1, 1)\n",
    "    ).flatten()\n",
    "    val_mse_denorm = mean_squared_error(val_targets_denorm, val_preds_denorm)\n",
    "    val_losses_denorm.append(val_mse_denorm)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Normalized   - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Denormalized - Train MSE: {train_mse_denorm:.4f}, Val MSE: {val_mse_denorm:.4f}\")\n",
    "    \n",
    "    # Early stopping based on denormalized validation MSE\n",
    "    if val_mse_denorm < best_val_loss:\n",
    "        best_val_loss = val_mse_denorm\n",
    "        patience_counter = 0\n",
    "        print(f\"  ‚Üí New best model! (Val MSE: {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation MSE (denormalized): {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "\n",
    "# Model is already trained, just set to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for validation set\n",
    "val_predictions = []\n",
    "val_actuals = []\n",
    "val_predictions_normalized = []\n",
    "val_actuals_normalized = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, scores in val_loader:\n",
    "        embeddings = embeddings.to(device)\n",
    "        scores_normalized = scores.to(device)\n",
    "        predictions_normalized = model(embeddings)\n",
    "        \n",
    "        # Store normalized predictions and actuals\n",
    "        val_predictions_normalized.extend(predictions_normalized.cpu().numpy())\n",
    "        val_actuals_normalized.extend(scores_normalized.cpu().numpy())\n",
    "\n",
    "# Convert to arrays\n",
    "val_predictions_normalized = np.array(val_predictions_normalized).reshape(-1, 1)\n",
    "val_actuals_normalized = np.array(val_actuals_normalized).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform to get original scale\n",
    "val_predictions = train_dataset.inverse_transform_score(val_predictions_normalized)\n",
    "val_actuals = train_dataset.inverse_transform_score(val_actuals_normalized)\n",
    "\n",
    "# Calculate metrics on original scale\n",
    "mse = mean_squared_error(val_actuals, val_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(val_actuals, val_predictions)\n",
    "r2 = r2_score(val_actuals, val_predictions)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION SET RESULTS (Original Scale)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Squared Error (MSE):  {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Also show normalized metrics for reference\n",
    "mse_norm = mean_squared_error(val_actuals_normalized, val_predictions_normalized)\n",
    "print(f\"\\nNormalized MSE: {mse_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61967623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_denorm, label='Training MSE', linewidth=2)\n",
    "plt.plot(val_losses_denorm, label='Validation MSE', linewidth=2)\n",
    "plt.axhline(y=8.5, color='r', linestyle='--', label='Baseline (8.5)', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE (Denormalized)', fontsize=12)\n",
    "plt.title('Training Progress (Original Scale)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (Normalized)', fontsize=12)\n",
    "plt.title('Training Loss (Normalized Scale)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../../output/training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ceba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Scatter plot: Predicted vs Actual\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(val_actuals, val_predictions, alpha=0.5, s=30)\n",
    "ax1.plot([val_actuals.min(), val_actuals.max()], \n",
    "         [val_actuals.min(), val_actuals.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Score', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Score', fontsize=12)\n",
    "ax1.set_title('Predicted vs Actual Scores', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.05, 0.95, f'R¬≤ = {r2:.4f}\\nRMSE = {rmse:.4f}', \n",
    "         transform=ax1.transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Residual plot\n",
    "ax2 = axes[0, 1]\n",
    "residuals = val_predictions - val_actuals\n",
    "ax2.scatter(val_actuals, residuals, alpha=0.5, s=30)\n",
    "ax2.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Actual Score', fontsize=12)\n",
    "ax2.set_ylabel('Residual (Predicted - Actual)', fontsize=12)\n",
    "ax2.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribution of residuals\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax3.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax3.set_xlabel('Residual', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Distribution of Residuals', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "ax3.text(0.05, 0.95, f'Mean = {residuals.mean():.4f}\\nStd = {residuals.std():.4f}', \n",
    "         transform=ax3.transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 4. Comparison of first N predictions\n",
    "ax4 = axes[1, 1]\n",
    "n_samples = min(50, len(val_actuals))\n",
    "indices = np.arange(n_samples)\n",
    "ax4.plot(indices, val_actuals[:n_samples], 'o-', label='Actual', linewidth=2, markersize=6)\n",
    "ax4.plot(indices, val_predictions[:n_samples], 's-', label='Predicted', linewidth=2, markersize=6)\n",
    "ax4.set_xlabel('Sample Index', fontsize=12)\n",
    "ax4.set_ylabel('Score', fontsize=12)\n",
    "ax4.set_title(f'First {n_samples} Predictions vs Actuals', fontsize=14, fontweight='bold')\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualizations saved to ../output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7841523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis on DENORMALIZED scale\n",
    "errors = np.abs(residuals)\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ERROR ANALYSIS (DENORMALIZED SCALE)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Median Absolute Error: {np.median(errors):.4f}\")\n",
    "print(f\"Max Absolute Error: {np.max(errors):.4f}\")\n",
    "print(f\"\\nError Percentiles:\")\n",
    "for p in percentiles:\n",
    "    print(f\"  {p}th percentile: {np.percentile(errors, p):.4f}\")\n",
    "\n",
    "# Find worst predictions\n",
    "worst_indices = np.argsort(errors)[-10:][::-1]\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"TOP 10 WORST PREDICTIONS (DENORMALIZED)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Index':<8} {'Actual':<12} {'Predicted':<12} {'Error':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for idx in worst_indices:\n",
    "    print(f\"{idx:<8} {val_actuals[idx]:<12.4f} {val_predictions[idx]:<12.4f} {errors[idx]:<12.4f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"COMPARISON TO BASELINE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Baseline MSE:        8.5000\")\n",
    "print(f\"Your Model MSE:      {mse:.4f}\")\n",
    "if mse < 8.5:\n",
    "    improvement = ((8.5 - mse) / 8.5) * 100\n",
    "    print(f\"Improvement:         {improvement:.1f}% better! üéâ\")\n",
    "else:\n",
    "    worse = ((mse - 8.5) / 8.5) * 100\n",
    "    print(f\"Performance:         {worse:.1f}% worse\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40baf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- TEMP BLOCK -------- #\n",
    "\n",
    "# Calculate optimal offset and MSE with correction\n",
    "offsets_to_test = np.arange(-10, 2, 0.5)  # Test offsets from -10 to +2\n",
    "mses = []\n",
    "\n",
    "for offset in offsets_to_test:\n",
    "    adjusted_predictions = val_predictions + offset\n",
    "    mse_adjusted = mean_squared_error(val_actuals, adjusted_predictions)\n",
    "    mses.append(mse_adjusted)\n",
    "\n",
    "# Find best offset\n",
    "best_idx = np.argmin(mses)\n",
    "best_offset = offsets_to_test[best_idx]\n",
    "best_mse = mses[best_idx]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OFFSET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Current MSE (no offset):     {mse:.4f}\")\n",
    "print(f\"Best offset:                 {best_offset:.4f}\")\n",
    "print(f\"MSE with best offset:        {best_mse:.4f}\")\n",
    "print(f\"Improvement:                 {mse - best_mse:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate simple mean offset\n",
    "mean_offset = (val_predictions - val_actuals).mean()\n",
    "print(f\"\\nMean prediction bias:        {mean_offset:.4f}\")\n",
    "print(f\"(Predictions are on average {abs(mean_offset):.2f} units {'higher' if mean_offset > 0 else 'lower'} than actuals)\")\n",
    "\n",
    "# Plot MSE vs offset\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(offsets_to_test, mses, linewidth=2)\n",
    "plt.axvline(x=best_offset, color='r', linestyle='--', label=f'Best offset: {best_offset:.2f}')\n",
    "plt.axhline(y=mse, color='g', linestyle='--', label=f'Current MSE: {mse:.2f}')\n",
    "plt.xlabel('Offset', fontsize=12)\n",
    "plt.ylabel('MSE', fontsize=12)\n",
    "plt.title('MSE vs Prediction Offset', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show corrected predictions\n",
    "val_predictions_corrected = val_predictions + best_offset\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(val_actuals[:50], 'o-', label='Actual', linewidth=2, markersize=6)\n",
    "plt.plot(val_predictions[:50], 's-', label='Predicted (original)', linewidth=2, markersize=6)\n",
    "plt.plot(val_predictions_corrected[:50], '^-', label=f'Predicted (offset {best_offset:.2f})', linewidth=2, markersize=6)\n",
    "plt.xlabel('Sample Index', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Predictions with Optimal Offset Correction', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate file to submit to kaggle\n",
    "\n",
    "# data at ../../../data/test/combined_test_data.pkl\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "with open('../../../data/test/combined_test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "if not isinstance(test_data, pd.DataFrame):\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "else:\n",
    "    test_df = test_data\n",
    "\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "print(f\"Test columns: {test_df.columns.tolist()}\")\n",
    "\n",
    "# Create test dataset (no scores, so we'll pass dummy scores)\n",
    "# class TestProteinScoreDataset(Dataset):\n",
    "#     def __init__(self, dataframe, score_scaler):\n",
    "#         self.df = dataframe.reset_index(drop=True)\n",
    "#         self.score_scaler = score_scaler\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # Get embeddings\n",
    "#         ref_embedding = row['ref_embedding']\n",
    "#         alt_embedding = row['alt_embedding']\n",
    "        \n",
    "#         # Ensure embeddings are torch tensors\n",
    "#         if not isinstance(ref_embedding, torch.Tensor):\n",
    "#             ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             ref_embedding = ref_embedding.float()\n",
    "            \n",
    "#         if not isinstance(alt_embedding, torch.Tensor):\n",
    "#             alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             alt_embedding = alt_embedding.float()\n",
    "        \n",
    "#         # Concatenate embeddings\n",
    "#         combined_embedding = torch.cat([ref_embedding, alt_embedding], dim=0)\n",
    "        \n",
    "#         # Normalize embeddings (L2 normalization)\n",
    "#         norm = torch.norm(combined_embedding)\n",
    "#         if norm > 0:\n",
    "#             combined_embedding = combined_embedding / norm\n",
    "        \n",
    "#         return combined_embedding, row['accession']\n",
    "\n",
    "# class TestProteinScoreDataset(Dataset):\n",
    "#     def __init__(self, dataframe, score_scaler):\n",
    "#         self.df = dataframe.reset_index(drop=True)\n",
    "#         self.score_scaler = score_scaler\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # Get embeddings\n",
    "#         ref_embedding = row['ref_embedding']\n",
    "#         alt_embedding = row['alt_embedding']\n",
    "        \n",
    "#         # Ensure embeddings are torch tensors\n",
    "#         if not isinstance(ref_embedding, torch.Tensor):\n",
    "#             ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             ref_embedding = ref_embedding.float()\n",
    "            \n",
    "#         if not isinstance(alt_embedding, torch.Tensor):\n",
    "#             alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "#         else:\n",
    "#             alt_embedding = alt_embedding.float()\n",
    "        \n",
    "#         # USE SAME DIFFERENCE APPROACH\n",
    "#         diff = alt_embedding - ref_embedding\n",
    "#         abs_diff = torch.abs(diff)\n",
    "#         combined_embedding = torch.cat([diff, abs_diff], dim=0)\n",
    "        \n",
    "#         # Normalize\n",
    "#         norm = torch.norm(combined_embedding)\n",
    "#         if norm > 0:\n",
    "#             combined_embedding = combined_embedding / norm\n",
    "        \n",
    "#         return combined_embedding, row['accession']\n",
    "\n",
    "# # Create test dataset and dataloader\n",
    "# test_dataset = TestProteinScoreDataset(test_df, train_dataset.score_scaler)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# print(f\"\\nGenerating predictions for {len(test_df)} test samples...\")\n",
    "\n",
    "# # Generate predictions\n",
    "# model.eval()\n",
    "# predictions_normalized = []\n",
    "# accessions = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, batch_accessions in tqdm(test_loader, desc=\"Predicting\"):\n",
    "#         embeddings = embeddings.to(device)\n",
    "        \n",
    "#         # Skip if NaN in embeddings\n",
    "#         if torch.isnan(embeddings).any() or torch.isinf(embeddings).any():\n",
    "#             # Use median prediction for problematic samples\n",
    "#             preds = torch.full((embeddings.size(0),), 0.5, device=device)\n",
    "#         else:\n",
    "#             preds = model(embeddings)\n",
    "            \n",
    "#             # Replace any NaN predictions with median\n",
    "#             if torch.isnan(preds).any():\n",
    "#                 preds = torch.where(torch.isnan(preds), torch.tensor(0.5, device=device), preds)\n",
    "        \n",
    "#         predictions_normalized.extend(preds.cpu().numpy())\n",
    "#         accessions.extend(batch_accessions)\n",
    "\n",
    "# # Convert to array and denormalize\n",
    "# predictions_normalized = np.array(predictions_normalized).reshape(-1, 1)\n",
    "# # In the test prediction loop, use the same Dataset class\n",
    "# # Then at the end:\n",
    "# predictions_denormalized = train_dataset.inverse_transform_score(all_predictions)\n",
    "\n",
    "# # Create submission dataframe with exact format from example\n",
    "# submission_df = pd.DataFrame({\n",
    "#     'accession': accessions,\n",
    "#     'score': predictions_denormalized  # Second column with empty header name\n",
    "# })\n",
    "\n",
    "# # Save to CSV\n",
    "# output_path = './output.csv'\n",
    "# submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"\\n‚úì Predictions saved to {output_path}\")\n",
    "# print(f\"\\nSubmission file preview:\")\n",
    "# print(submission_df.head(10))\n",
    "# print(f\"\\nPrediction statistics:\")\n",
    "# print(f\"Min: {predictions_denormalized.min():.4f}\")\n",
    "# print(f\"Max: {predictions_denormalized.max():.4f}\")\n",
    "# print(f\"Mean: {predictions_denormalized.mean():.4f}\")\n",
    "# print(f\"Median: {np.median(predictions_denormalized):.4f}\")\n",
    "\n",
    "# Test dataset using the SAME feature construction as training\n",
    "\n",
    "class TestProteinScoreDataset(Dataset):\n",
    "    def __init__(self, dataframe, cat_columns, cat_maps):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.cat_columns = cat_columns\n",
    "        self.cat_maps = cat_maps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Embeddings\n",
    "        ref_embedding = row['ref_embedding']\n",
    "        alt_embedding = row['alt_embedding']\n",
    "        \n",
    "        if not isinstance(ref_embedding, torch.Tensor):\n",
    "            ref_embedding = torch.tensor(ref_embedding, dtype=torch.float32)\n",
    "        else:\n",
    "            ref_embedding = ref_embedding.float()\n",
    "            \n",
    "        if not isinstance(alt_embedding, torch.Tensor):\n",
    "            alt_embedding = torch.tensor(alt_embedding, dtype=torch.float32)\n",
    "        else:\n",
    "            alt_embedding = alt_embedding.float()\n",
    "        \n",
    "        # diff + |diff|, normalized\n",
    "        diff = alt_embedding - ref_embedding\n",
    "        abs_diff = torch.abs(diff)\n",
    "        combined_embedding = torch.cat([diff, abs_diff], dim=0)\n",
    "        \n",
    "        norm = torch.norm(combined_embedding)\n",
    "        if norm > 0:\n",
    "            combined_embedding = combined_embedding / norm\n",
    "\n",
    "        # Extra features (same as train)\n",
    "        cat_feats = []\n",
    "        for col in self.cat_columns:\n",
    "            val = str(row[col])\n",
    "            mapping = self.cat_maps.get(col, {})\n",
    "            cat_feats.append(mapping.get(val, -1))\n",
    "\n",
    "        pos_val = row['pos']\n",
    "        if pd.isna(pos_val) or np.isinf(pos_val):\n",
    "            pos_val = -1\n",
    "        extra_features = torch.tensor(cat_feats + [float(pos_val)],\n",
    "                                      dtype=torch.float32)\n",
    "\n",
    "        X = torch.cat([combined_embedding, extra_features], dim=0)\n",
    "\n",
    "        return X, row['accession']\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "test_dataset = TestProteinScoreDataset(test_df, cat_columns, cat_maps)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nGenerating predictions for {len(test_df)} test samples...\")\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "predictions_normalized = []\n",
    "accessions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, batch_accessions in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        if torch.isnan(X_batch).any() or torch.isinf(X_batch).any():\n",
    "            preds = torch.full((X_batch.size(0),), 0.5, device=device)\n",
    "        else:\n",
    "            preds = model(X_batch)\n",
    "            if torch.isnan(preds).any():\n",
    "                preds = torch.where(\n",
    "                    torch.isnan(preds),\n",
    "                    torch.tensor(0.5, device=device),\n",
    "                    preds\n",
    "                )\n",
    "        \n",
    "        predictions_normalized.extend(preds.cpu().numpy())\n",
    "        accessions.extend(batch_accessions)\n",
    "\n",
    "predictions_normalized = np.array(predictions_normalized).reshape(-1, 1)\n",
    "\n",
    "# Denormalize to original score scale\n",
    "predictions_denormalized = train_dataset.inverse_transform_score(predictions_normalized)\n",
    "\n",
    "# Submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'accession': accessions,\n",
    "    'score': predictions_denormalized\n",
    "})\n",
    "\n",
    "output_path = './output.csv'\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Predictions saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f69167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check what's actually happening\n",
    "print(\"VALIDATION SET ANALYSIS:\")\n",
    "print(f\"Val predictions (denorm) - Min: {val_predictions.min():.4f}, Max: {val_predictions.max():.4f}, Mean: {val_predictions.mean():.4f}\")\n",
    "print(f\"Val actuals (denorm) - Min: {val_actuals.min():.4f}, Max: {val_actuals.max():.4f}, Mean: {val_actuals.mean():.4f}\")\n",
    "print(f\"Val MSE: {mse:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET PREDICTIONS (from submission file):\")\n",
    "# Load your submission file\n",
    "test_submission = pd.read_csv('./output.csv')\n",
    "print(f\"Test predictions - Min: {test_submission.iloc[:, 1].min():.4f}, Max: {test_submission.iloc[:, 1].max():.4f}, Mean: {test_submission.iloc[:, 1].mean():.4f}\")\n",
    "print(f\"Test predictions first 10:\")\n",
    "print(test_submission.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING DATA SCORE RANGE:\")\n",
    "print(f\"Train scores - Min: {df['score'].min():.4f}, Max: {df['score'].max():.4f}, Mean: {df['score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03948a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if accessions are in the right order\n",
    "print(\"TEST DATASET - First 10 accessions from pkl file:\")\n",
    "print(test_df['accession'].head(10).tolist())\n",
    "\n",
    "print(\"\\nTEST PREDICTIONS - First 10 accessions from submission file:\")\n",
    "print(test_submission['score'].head(10).tolist())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DO THEY MATCH?\")\n",
    "match = (test_df['accession'].values == test_submission['accession'].values).all()\n",
    "print(f\"All accessions match: {match}\")\n",
    "\n",
    "if not match:\n",
    "    print(\"\\n‚ö†Ô∏è MISMATCH FOUND!\")\n",
    "    print(\"Finding first mismatch...\")\n",
    "    for i in range(min(50, len(test_df))):\n",
    "        if test_df['accession'].iloc[i] != test_submission['accession'].iloc[i]:\n",
    "            print(f\"Index {i}:\")\n",
    "            print(f\"  Expected: {test_df['accession'].iloc[i]}\")\n",
    "            print(f\"  Got:      {test_submission['score'].iloc[i]}\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
